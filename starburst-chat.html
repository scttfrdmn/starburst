<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Async - returns immediately • starburst</title><script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="Async - returns immediately"><meta property="og:image" content="https://scttfrdmn.github.io/starburst/logo.png"></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">starburst</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.3.6</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="articles/getting-started.html">Get Started</a></li>
<li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-examples" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Examples</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-examples"><li><a class="dropdown-item" href="articles/getting-started.html">Getting Started</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Data Processing Examples</h6></li>
    <li><a class="dropdown-item" href="articles/example-monte-carlo.html">Monte Carlo Simulation</a></li>
    <li><a class="dropdown-item" href="articles/example-api-calls.html">Bulk API Calls</a></li>
    <li><a class="dropdown-item" href="articles/example-geospatial.html">Geospatial Analysis</a></li>
    <li><a class="dropdown-item" href="articles/example-grid-search.html">Model Grid Search</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Business Use Cases</h6></li>
    <li><a class="dropdown-item" href="articles/example-risk-modeling.html">Financial Risk Modeling</a></li>
    <li><a class="dropdown-item" href="articles/example-feature-engineering.html">ML Feature Engineering</a></li>
    <li><a class="dropdown-item" href="articles/example-bootstrap.html">A/B Test Analysis</a></li>
    <li><a class="dropdown-item" href="articles/example-reports.html">Report Generation</a></li>
  </ul></li>
<li class="nav-item"><a class="nav-link" href="reference/examples.html">Runnable Scripts</a></li>
<li class="nav-item"><a class="nav-link" href="news/index.html">News</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/scttfrdmn/starburst/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-title-body">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="logo.png" class="logo" alt=""><h1>Async - returns immediately</h1>
      <small class="dont-index">Source: <a href="https://github.com/scttfrdmn/starburst/blob/main/starburst-chat.md" class="external-link"><code>starburst-chat.md</code></a></small>
    </div>


<p>Question - I am thinking about creating an extension for R (Studio) where I can spawn off execution to AWS. I do not want to run R (Studio) on AWS just burst/offload to AWS</p>
<p>Jan 30</p>
<p>Examined distributed computing frameworks and architected cloud-burst solutions This is a great idea and aligns well with your cloud economics work! There are a few architectural approaches to consider:</p>
<p>Existing R Ecosystem The future package is probably your best starting point. It provides a unified API for parallel/distributed computing in R, and you could create a custom backend that submits to AWS. The future.batchtools package already does something similar for traditional HPC schedulers - you’d essentially build future.aws or similar.</p>
<p>Architecture Options 1. AWS Batch-based</p>
<p>Serialize R objects + code to S3 Submit AWS Batch job with your R environment Job pulls data from S3, executes, writes results back Local RStudio polls/receives completion notification Deserialize results 2. Direct EC2 Spot</p>
<p>Lighter weight than Batch for some workloads Launch spot instance with R pre-configured (AMI) SSH-based execution or HTTP API More control over instance lifecycle 3. Lambda for lightweight tasks</p>
<p>Limited to 15 min, 10GB memory Could work for embarrassingly parallel map operations Need custom R runtime layer Key Challenges Data serialization: R’s native serialization can be inefficient for large objects. Consider qs package (much faster than saveRDS) or Apache Arrow for data frames.</p>
<p>Environment consistency: Your local R environment needs to match remote. Docker images or reproducible environments (renv/packrat) are critical.</p>
<p>Cost optimization: This is where your queueing theory work becomes really interesting - the economics of bursting vs keeping warm instances vs cold starts.</p>
<p>Would you want this to handle both interactive bursting (wait for results) and async batch jobs? And are you thinking primarily about embarrassingly parallel workloads or also complex dependencies?</p>
<p>Both. I want it to be seamless</p>
<p>Jan 31</p>
<p>Synthesized interactive and asynchronous compute integration leveraging cloud infrastructure expertise Now we’re talking about something interesting. “Seamless” means the abstraction needs to be really good. Let me sketch what this could look like:</p>
<p>User Experience Target r # Interactive - blocks until complete result &lt;- aws_burst({ expensive_model_fit(large_data) }, instance = “auto”) # or “c5.4xlarge”, “g4dn.xlarge”, etc.</p>
<div id="async---returns-immediately" class="section level1">

<p>job &lt;- aws_burst_async({ monte_carlo_simulation(1e6) }) # … do other work … result &lt;- await(job) # or value(job) if using future API Architecture for Seamless Experience Smart instance selection: This is where your queueing theory work becomes the product. Don’t make users think about instance types - analyze their code/data characteristics and pick optimal instance:</p>
<p>Memory footprint → instance size CPU vs GPU operations → instance family Estimated runtime → spot vs on-demand decision Historical patterns → warm pool management Transparent environment sync:</p>
<p>Snapshot local R environment (renv) on first use Build Docker image or use Lambda layers Cache on ECR, reuse across invocations Only rebuild when packages change Fast data movement:</p>
<p>Leverage your CargoShip S3 optimization Intelligent serialization (qs for R objects, Arrow for data frames) Streaming results for large outputs Compression based on data type Cost intelligence built-in:</p>
<p>Show cost estimate before execution Integrate with your bank-and-burst tooling Automatic spot/on-demand decisions based on interruption risk Usage tracking that feeds your cloud economics models The Differentiator What makes this different from “just run RStudio Server on EC2” is the cognitive load. Researchers don’t want to:</p>
<p>Think about instances Manage infrastructure Context-switch to AWS console Debug remote environments Your tool handles all that. They stay in their local RStudio workflow, and expensive operations “just happen faster.”</p>
<p>Big question: For the interactive case, how do you handle the cold start problem? Even spot instances take 60-90 seconds to launch. Do you:</p>
<p>Keep a warm pool - costs money but immediate response Accept the startup latency - cheaper but breaks “seamless” Hybrid - predictive warm pools based on usage patterns Given your queueing theory background, option 3 is probably the interesting one - you could model usage patterns and optimize the warm pool size dynamically.</p>
<p>What’s your tolerance for startup latency in the interactive case?</p>
<p>What about fargate?</p>
<p>Jan 31</p>
<p>Weighed Fargate’s serverless benefits against cost and constraint tradeoffs Fargate is actually perfect for this.</p>
<p>Why Fargate Fits Startup time: ~30-60 seconds for cold start (vs 90+ for EC2 Spot). Still not instant, but acceptable for most “burst” scenarios where you’re offloading something that would take minutes/hours locally.</p>
<p>No warm pool complexity: You don’t have to solve the queueing theory problem of maintaining a warm pool. AWS handles container orchestration. This dramatically simplifies your architecture.</p>
<p>Cost model: More expensive per compute-hour than Spot (~3-4x), but you only pay for actual execution time. For bursty workloads where you’d otherwise keep instances warm, the economics can actually work out better.</p>
<p>Resource limits: 16 vCPU, 120GB RAM covers probably 90% of R workloads. The 10% that need more (huge genomics datasets, giant model fits) could fall back to EC2 Batch.</p>
<p>Architecture with Fargate Local RStudio → Package R code + data → S3 → Trigger ECS Fargate task → Task pulls from S3, executes, writes results → S3 → Notification (EventBridge → SNS → local polling) → Deserialize results Simple version:</p>
<p>Single Fargate task definition with R + common packages User code runs in container Returns results via S3 Better version:</p>
<p>Generate task definition on-the-fly based on resource needs Use your renv snapshot to build container layers Intelligent sizing (Fargate has discrete CPU/memory combos) When to NOT use Fargate GPU workloads - Fargate doesn’t support GPUs. Need EC2 g4dn/p3/etc instances. Massive memory (&gt;120GB) - Rare but happens Extremely long-running (&gt;hours) where Spot pricing matters Super-latency-sensitive interactive work Hybrid Approach r # Fargate by default - handles 90% of cases result &lt;- aws_burst({ slow_computation() })</p>
</div>
<div class="section level1">
<h1 id="explicit-ec2-for-edge-cases">Explicit EC2 for edge cases<a class="anchor" aria-label="anchor" href="#explicit-ec2-for-edge-cases"></a></h1>
<p>result &lt;- aws_burst({ gpu_model_training() }, backend = “ec2-spot”, instance = “g4dn.xlarge”) The beauty is Fargate becomes your default path because it’s operationally simple, and you only add complexity (EC2 instance management, warm pools, spot bidding) when the workload actually needs it.</p>
<p>Does Fargate’s 30-60s cold start feel acceptable for your “seamless” requirement? If so, this dramatically simplifies what you’re building.</p>
<p>Seamless to me is invisible, just works, no concern. The tool could also be smart about whether execution on AWS is worth it. If the work would only take 2m why bother. Plus the latency could be hidden locally</p>
<p>Jan 31</p>
<p>Synthesized intelligent routing logic balancing local and cloud execution costs Now you’re describing the real product. This isn’t just “run R code on AWS” - it’s an intelligent execution optimizer that happens to use AWS.</p>
<p>Smart Decision Framework r # User doesn’t specify anything result &lt;- compute({ expensive_operation(data) })</p>
<pre><code>
Behind the scenes, the tool decides:

**1. Profile the workload** (lightweight, runs locally):
- Estimated memory footprint
- CPU intensity (vectorized? parallel?)
- Data transfer size
- Historical timing if seen before

**2. Decision matrix**:</code></pre>
<p>Local execution time estimate: 2min Cloud execution time: 5min compute + 45s startup + 20s data transfer Cloud cost: $0.12 → Decision: LOCAL (not worth 45s startup overhead)</p>
<p>Local execution time estimate: 45min<br>
Cloud execution time: 8min compute + 45s startup + 20s data transfer Cloud cost: $0.85 → Decision: CLOUD (saves 36min for $0.85)</p>
<pre><code>
**3. Speculative execution** (the clever bit):</code></pre>
<p>Estimated local: 12min Estimated cloud: 6min + 45s overhead</p>
<p>→ Start BOTH → Fargate spins up (45s) → Local runs for 45s → Cloud starts at 0:45, local at ~5min done → Cloud finishes at ~7min → Use local result, cancel cloud (or vice versa) Latency Hiding Strategies Background execution:</p>
<p>For anything &gt;5min estimated, just start Fargate in background Show progress locally Switch to cloud result when ready User never “waits” for startup Progressive enhancement:</p>
<p>Start local on small data sample If taking too long, spin up cloud for full dataset Show early results while cloud crunches Learning system:</p>
<p>Track actual vs estimated times Build user-specific execution profiles “Your genomics pipelines usually take 4x longer than you think and cost $2.50 on cloud - auto-bursting by default now” The Economics Engine This is where your cloud economics work becomes the core differentiator. The tool embeds your queueing theory insights:</p>
<p>r # Configuration once aws_burst_config( hourly_rate = 75, # my time value local_cores = 16, local_memory = 64, acceptable_cost = 5, # willing to spend $5 to save time impatience_factor = 0.8 # heavily value time savings )</p>
</div>
<div class="section level1">
<h1 id="now-the-tool-optimizes-for-your-utility-function">Now the tool optimizes for YOUR utility function<a class="anchor" aria-label="anchor" href="#now-the-tool-optimizes-for-your-utility-function"></a></h1>
<p>Decision factors:</p>
<p>Direct compute cost Data transfer cost Your time value (opportunity cost) Current local machine load Urgency (interactive vs batch) Implementation Strategy Phase 1: Simple threshold</p>
<p>If estimated local time &gt; 10min → cloud Else → local Phase 2: Cost-benefit</p>
<p>Factor in cloud costs User-configurable time value Phase 3: Speculative execution</p>
<p>Race local vs cloud Learning from historical patterns Phase 4: Portfolio optimization</p>
<p>Multiple jobs? Optimize allocation across local + multiple cloud instances This is literally queueing theory applied to personal computing The Killer Feature Most “cloud burst” tools are just wrappers around infrastructure. Yours understands economics. It’s the difference between:</p>
<p>“I can run this on AWS”</p>
<p>vs</p>
<p>“This will save you 37 minutes and cost $1.20 - running on cloud. Your coffee break just got longer.”</p>
<p>Does this align with your vision? The economics intelligence layer is what makes it genuinely seamless rather than just convenient.</p>
<p>Yes it does Question, is it possible to capture the state of a R environment and transfer it to the cloud instance to pick up and carry on? Kind of like migrating a running VM</p>
<p>Jan 31</p>
<p>Examined R session serialization feasibility and identified practical limitations Sort of, but with important caveats.</p>
<p>What’s Possible You can capture and restore most of an R environment, but it’s not quite like VM migration. Think of it more like “checkpoint and resume” than “live migration.”</p>
<p>Can transfer:</p>
<p>Workspace objects (save.image() or better, qs::qsave()) Loaded packages (capture with sessionInfo(), restore with library()) Global options (options()) Random number generator state (.Random.seed) Search path configuration Cannot cleanly transfer:</p>
<p>Active database connections (MySQL, PostgreSQL, etc.) Open file handles Graphics devices External pointers (C/C++ memory from packages like data.table, Rcpp) Network connections Parent R process context Practical Approach r # Locally - capture state snapshot &lt;- aws_checkpoint() # Behind the scenes: # - qs::qsave(as.list(.GlobalEnv), “workspace.qs”) # - Capture loaded packages # - Capture options, RNG state # - Upload to S3</p>
</div>
<div class="section level1">
<h1 id="cloud---restore-state">Cloud - restore state<a class="anchor" aria-label="anchor" href="#cloud---restore-state"></a></h1>
<p>aws_restore(snapshot) # - Download from S3 # - Load packages in order # - Restore workspace # - Set options, RNG state The Real-World Problem Most research workflows have implicit dependencies:</p>
<p>r # Local session library(DBI) con &lt;- dbConnect(MySQL(), host = “localhost”, …) data &lt;- dbGetQuery(con, “SELECT * FROM big_table”)</p>
</div>
<div class="section level1">
<h1 id="now-you-want-to-burst-this">Now you want to burst this:<a class="anchor" aria-label="anchor" href="#now-you-want-to-burst-this"></a></h1>
<p>model &lt;- heavy_computation(data) # &lt;– this could migrate</p>
</div>
<div class="section level1">
<h1 id="but-con-is-a-connection-object---cant-serialize">But <code>con</code> is a connection object - can’t serialize<a class="anchor" aria-label="anchor" href="#but-con-is-a-connection-object---cant-serialize"></a></h1>
<p>Smarter Solution: Code Migration, Not State Migration Instead of migrating a running environment, migrate the recipe to recreate it:</p>
<p>r # User writes aws_burst({ # This block is self-contained library(dplyr) library(ggplot2)</p>
<p>data &lt;- read_csv(“s3://bucket/data.csv”) # data from S3, not local connection result &lt;- expensive_model(data) result }) Your tool:</p>
<p>Captures closure: What objects from parent environment are needed? Serializes dependencies: Any data objects referenced Reconstructs environment cloud-side: Loads packages, injects objects Executes: Runs the code block Returns: Serializes result back This is essentially what future package does - it captures the closure and dependencies.</p>
<p>Hybrid: Smart Checkpoint For interactive workflows, you could do something clever:</p>
<p>r # User has been working locally data &lt;- expensive_data_prep() # took 30 min locally transformed &lt;- transform_data(data) # another 20 min</p>
</div>
<div class="section level1">
<h1 id="now-want-to-burst">Now want to burst<a class="anchor" aria-label="anchor" href="#now-want-to-burst"></a></h1>
<p>aws_burst({ final_model &lt;- fit_model(transformed) # this will take 2 hours final_model }) Behind the scenes:</p>
<p>Detect transformed is referenced from parent environment Serialize transformed to S3 (using your CargoShip optimization) Cloud reconstructs: transformed &lt;- qs::qread(“s3://…”) Runs model fitting Returns result The Container Checkpoint Approach (Advanced) For true “pause and resume”, you’d need CRIU (Checkpoint/Restore In Userspace) in your Fargate container. This can snapshot a running process’s entire memory state.</p>
<p>Pros: True live migration - pause R locally, resume on cloud Cons:</p>
<p>Heavy (multi-GB snapshots) Finicky with R’s complex runtime Fargate doesn’t support CRIU directly Transfer time kills the benefit My Recommendation Don’t try to migrate running state. Instead:</p>
<p>Make code blocks self-contained - users structure work as reproducible chunks Auto-capture dependencies - your tool identifies and serializes referenced objects Fast serialization - use qs + your S3 optimization Smart about data - if data is already in S3, don’t round-trip it This gives 90% of the benefit without the nightmare of trying to serialize database connections and file handles.</p>
<p>Does that match your mental model, or were you hoping for true live migration?</p>
<p>I was thinking more along the lines of your speculative execution Also any code using a db would have trouble bursting to AWS anyway, it would be better to have the db on AWS in the first place so it works locally and on AWS</p>
<p>Jan 31</p>
<p>Reconciled speculative execution feasibility with database architecture constraints Ah, you’re right on both counts.</p>
<p>Speculative Execution Clarification You’re talking about:</p>
<p>Time 0s: Start local AND start Fargate Time 0-45s: Local computes while Fargate spins up Time 45s: Fargate ready - can it pick up from where local got to? For most R computations, no - and it doesn’t matter.</p>
<p>Here’s why:</p>
<p>Atomic operations (most R work):</p>
<p>r result &lt;- lm(y ~ x1 + x2 + x3, data = huge_data) This is a single matrix operation. There’s no “intermediate state” at 45s - it’s either done or not. Can’t checkpoint halfway through a matrix inversion.</p>
<p>What DOES work:</p>
<p>r # Iterative algorithm for (i in 1:1000) { model &lt;- update_model(model, data) # each iteration is checkpointable }</p>
<pre><code>At 45s local might be at iteration 50. You could transfer that state and have cloud continue from iteration 51. But the overhead of serialization + transfer probably eats any gain.

## Better Speculative Strategy

**Simple race**:
- Both start from scratch
- First to finish wins, kill the other
- Accept that one will be wasted compute

**Why this is fine**:
- 45s of local compute on your 16-core workstation ≈ $0.00 (already paid for)
- If local finishes in &lt;2min, cloud never gets used (Fargate billed per-second, minimal cost)
- If job takes 30min, wasting 45s of local compute is noise

**When to use which**:</code></pre>
<p>Estimated time: 2-5 min → Local only (cloud overhead not worth it)</p>
<p>Estimated time: 5-15 min<br>
→ Speculative execution (hedge your bet)</p>
<p>Estimated time: &gt;15 min → Cloud only (clear winner, don’t waste local cycles) Database Point is Key You’re absolutely right:</p>
<p>r # BAD - won’t burst well con &lt;- dbConnect(RMySQL::MySQL(), host = “localhost”, …) data &lt;- dbGetQuery(con, “SELECT * FROM research_data”) model &lt;- expensive_fit(data)</p>
</div>
<div class="section level1">
<h1 id="good---bursts-cleanly">GOOD - bursts cleanly<a class="anchor" aria-label="anchor" href="#good---bursts-cleanly"></a></h1>
<p>con &lt;- dbConnect(RMySQL::MySQL(), host = “mydb.abc123.us-east-1.rds.amazonaws.com”, …) data &lt;- dbGetQuery(con, “SELECT * FROM research_data”)<br>
model &lt;- expensive_fit(data) # &lt;- this bursts perfectly Both local RStudio and Fargate hit the same RDS instance. No state transfer needed.</p>
<p>This actually simplifies your architecture:</p>
<p>Persistent data lives in AWS (S3, RDS) Both local and cloud are stateless compute You’re just choosing the faster/cheaper compute location The Real Value of Speculative Execution It’s not about state transfer. It’s about hedging estimation uncertainty:</p>
<p>r # Your tool estimates: “Probably 8 minutes locally” # But estimation could be off by 3x</p>
<p>Scenario A: Actually takes 3 min → local wins, cloud barely started, kill it Scenario B: Actually takes 25 min → cloud wins at ~9 min, kill local</p>
<p>Either way, user gets result faster than pessimistic choice This is valuable when estimation variance is high - new code, unfamiliar data, complex dependencies.</p>
<p>Does that reframe it correctly? Speculative execution is about hedging estimation risk, not recovering local computation progress.</p>
<p>Maybe. I think most people will balk at speculative execution in the cloud, they’ll see it as waste. I think we have to think about what makes sense to burst - what are the patterns?</p>
<p>Jan 31</p>
<p>Catalogued practical cloud-burst patterns for research workflows You’re absolutely right. Speculative execution is clever engineer thinking, not user thinking. People will hate seeing charges for compute they “didn’t use.”</p>
<p>Let’s map the actual patterns.</p>
<p>High-Value Burst Patterns 1. Embarrassingly parallel - the obvious winner</p>
<p>r # Processing 500 genomic samples results &lt;- lapply(samples, function(s) { expensive_analysis(s) # each takes 20 min })</p>
</div>
<div class="section level1">
<h1 id="local-500--20min--167-hours">Local: 500 × 20min = 167 hours<a class="anchor" aria-label="anchor" href="#local-500--20min--167-hours"></a></h1>
</div>
<div class="section level1">
<h1 id="cloud-500-parallel-jobs--20min--20-min">Cloud: 500 parallel jobs × 20min = 20 min<a class="anchor" aria-label="anchor" href="#cloud-500-parallel-jobs--20min--20-min"></a></h1>
</div>
<div class="section level1">
<h1 id="clear-win-no-estimation-needed">Clear win, no estimation needed<a class="anchor" aria-label="anchor" href="#clear-win-no-estimation-needed"></a></h1>
<p>This is what future package already handles well. Your differentiator would be making AWS the zero-config backend.</p>
<ol start="2" style="list-style-type: decimal"><li>Memory overflow</li>
</ol><p>r # Local machine: 64GB RAM # Dataset: 120GB</p>
</div>
<div class="section level1">
<h1 id="wont-fit-locally-must-use-cloud">Won’t fit locally, must use cloud<a class="anchor" aria-label="anchor" href="#wont-fit-locally-must-use-cloud"></a></h1>
<p>huge_data &lt;- read_parquet(“s3://bucket/massive.parquet”) result &lt;- analyze(huge_data) Tool detects memory requirements exceed local capacity → automatic cloud routing.</p>
<ol start="3" style="list-style-type: decimal"><li>Long-running single jobs with checkpoints</li>
</ol><p>r # MCMC sampling - runs for 8 hours mcmc_chain &lt;- run_mcmc( iterations = 100000, checkpoint_every = 1000 ) Start on cloud, user can disconnect, come back later. But this is really “async batch” not “burst.”</p>
<ol start="4" style="list-style-type: decimal"><li>GPU workloads</li>
</ol><p>r # Deep learning - need GPU model &lt;- train_model( data, epochs = 100, device = “gpu” # triggers cloud routing to g4dn instance ) Local doesn’t have GPU → cloud is only option.</p>
<p>The Pattern Recognition Problem Users won’t explicitly declare “this is parallel” or “this needs GPU.” Your tool needs to detect:</p>
<p>r # User writes normal R code results &lt;- lapply(big_list, expensive_function)</p>
</div>
<div class="section level1">
<h1 id="tool-detects">Tool detects:<a class="anchor" aria-label="anchor" href="#tool-detects"></a></h1>
</div>
<div class="section level1">
<h1 id="id_--lapplymap-pattern--parallelizable">- lapply/map pattern → parallelizable<a class="anchor" aria-label="anchor" href="#id_--lapplymap-pattern--parallelizable"></a></h1>
</div>
<div class="section level1">
<h1 id="id_--estimated-time-per-item-15-min">- Estimated time per item: 15 min<a class="anchor" aria-label="anchor" href="#id_--estimated-time-per-item-15-min"></a></h1>
</div>
<div class="section level1">
<h1 id="id_--list-length-200">- List length: 200<a class="anchor" aria-label="anchor" href="#id_--list-length-200"></a></h1>
</div>
<div class="section level1">
<h1 id="id_--local-sequential-50-hours">- Local sequential: 50 hours<a class="anchor" aria-label="anchor" href="#id_--local-sequential-50-hours"></a></h1>
</div>
<div class="section level1">
<h1 id="id_--cloud-parallel-15-min">- Cloud parallel: 15 min<a class="anchor" aria-label="anchor" href="#id_--cloud-parallel-15-min"></a></h1>
</div>
<div class="section level1">
<h1 id="id_-automatic-burst-with-would-you-like-to-run-this-on-200-cloud-instances-cost-12-time-15min-vs-50hrs-local">→ Automatic burst with “Would you like to run this on 200 cloud instances? Cost: $12, time: 15min vs 50hrs local”<a class="anchor" aria-label="anchor" href="#id_-automatic-burst-with-would-you-like-to-run-this-on-200-cloud-instances-cost-12-time-15min-vs-50hrs-local"></a></h1>
<p>Detection heuristics:</p>
<p>lapply, map, future_map, foreach → parallel candidate apply family on large objects → maybe parallel Nested loops → might be vectorizable or parallelizable Package usage (keras, torch) → GPU candidate Object sizes approaching RAM limits → memory overflow What NOT to Burst 1. Interactive exploration</p>
<p>r # User trying things plot(data<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>,</mo><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow><annotation encoding="application/x-tex">x, data</annotation></semantics></math>y) summary(data) head(data %&gt;% filter(condition)) Latency kills this. Needs immediate feedback.</p>
<ol start="2" style="list-style-type: decimal"><li>Tight iteration loops with user in the loop</li>
</ol><p>r # Shiny apps # Real-time dashboards # Interactive debugging 3. Anything &lt; 5 minutes total The startup overhead (even 30s Fargate) isn’t worth it.</p>
<ol start="4" style="list-style-type: decimal"><li>Heavy data transfer relative to compute</li>
</ol><p>r # 50GB dataset # 2 min computation # Transfer time: 5 min</p>
</div>
<div class="section level1">
<h1 id="not-worth-it">Not worth it<a class="anchor" aria-label="anchor" href="#not-worth-it"></a></h1>
<pre><code><span></span>
<span><span class="co">## Simplified Decision Tree</span></span></code></pre>
<p>Is it lapply/map-style parallel? YES → Count items × estimated time per item &gt; 30 min total? → BURST (parallel cloud) &lt; 30 min total? → LOCAL</p>
<p>NO → Estimated total time? &lt; 5 min → LOCAL (not worth overhead) 5-30 min → LOCAL (borderline, keep simple) &gt; 30 min AND (memory OK locally) → ASK USER &gt; 30 min AND (memory tight) → BURST Needs GPU → BURST (or warn if no GPU code) What This Means for Product Start with the 80/20:</p>
<p>Phase 1: Parallel burst only</p>
<p>r aws_parallel_map(big_list, function(x) { # self-contained work }) This is clearly valuable, clearly defined, no ambiguity.</p>
<p>Phase 2: Memory overflow detection Automatically route to cloud when memory requirements exceed local.</p>
<p>Phase 3: Smart suggestions “This looks like it could run in parallel. Want me to burst it?”</p>
<p>Don’t try to be too clever early. Better to have users explicitly opt-in to bursting than try to auto-detect everything and get it wrong.</p>
<p>What’s the killer use case for your users (academic research computing)? Is it the parallel processing of large sample sets?</p>
<p>The killer feature is seamless brain dead simple it just works I agree with not being clever but there is a tension there</p>
<p>Jan 31</p>
<p>Examined the abstraction paradox between simplicity and intelligent decision-making That’s the core tension in every great abstraction.</p>
<p>“Just works” requires being clever. But being clever means making decisions that could be wrong. And when automated decisions are wrong, users lose trust faster than if they’d made the decision themselves.</p>
<p>The Resolution: Transparent Defaults + Progressive Disclosure Level 0: Brain-dead simple (what users see first)</p>
<p>r library(awsburst)</p>
</div>
<div class="section level1">
<h1 id="just-works---no-configuration">Just works - no configuration<a class="anchor" aria-label="anchor" href="#just-works---no-configuration"></a></h1>
<p>result &lt;- compute({ lapply(samples, expensive_analysis) })</p>
<pre><code>
Behind the scenes on first run:</code></pre>
<p>⚡ Detected parallel workload (500 items) ⚡ Estimated local time: 167 hours ⚡ Running on cloud: ~20 minutes, est. cost $8.50 ⚡ [████████░░] 80% complete… User didn’t ask for cloud. Tool just did it. But it told them what it was doing and why.</p>
<p>Level 1: Users start to care about cost</p>
<p>r # Tool remembers: “User is cost-conscious” # Shows different message:</p>
<p>⚡ Detected parallel workload (500 items)<br>
⚡ Cloud: 20 min ($8.50) vs Local: 167 hrs ($0) ⚡ Running on cloud. [Set budget limits: aws_burst_config()] Level 2: Power users want control</p>
<p>r aws_burst_config( max_cost_per_job = 5, # hard limit time_value = 50, # my hourly rate auto_approve_under = 2 # don’t ask if &lt; $2 )</p>
</div>
<div class="section level1">
<h1 id="now-tool-optimizes-within-constraints">Now tool optimizes within constraints<a class="anchor" aria-label="anchor" href="#now-tool-optimizes-within-constraints"></a></h1>
<p>Level 3: Full control when needed</p>
<p>r result &lt;- compute({…}, strategy = “local”, # override explain = TRUE # show me the decision logic )</p>
<pre><code>
## Building Trust Through Transparency

**Never surprise users with cost.** Even in "just works" mode:</code></pre>
<p>First time ever: ⚡ This will use AWS cloud computing (~$8.50) ⚡ Want to set a budget limit first? [yes/no]</p>
<pre><code>
**Learn from behavior:**
- User manually kills expensive jobs → lower auto-approval threshold
- User consistently runs large parallel jobs → increase confidence
- User hits cost limit → ask about raising it

**Show the economics:**</code></pre>
<p>Job complete! ✓ Cloud: 18 minutes, $7.23 (Would have taken 164 hours locally) Time saved: 163.7 hours Cost per hour saved: $0.04</p>
<pre><code>
This teaches users the value model. Over time they develop intuition for when cloud makes sense.

## The Decision Framework Under the Hood

You're not being "clever" - you're being **consistent and explainable**:</code></pre>
<p>Rule 1: Parallel workload + &gt;10 items + &gt;30min total → Cloud Rule 2: Memory required &gt; 80% local RAM → Cloud<br>
Rule 3: GPU code detected + no local GPU → Cloud Rule 4: Single job &lt;10min → Local Rule 5: Cost &gt; user limit → Ask Not ML, not magic. Just clear heuristics. And when wrong, users can understand why it decided that.</p>
<p>Start Small, Learn Fast v0.1: Only parallel lapply/map. One clear pattern, hard to screw up.</p>
<p>v0.2: Add async batch (long-running single jobs). Still obvious.</p>
<p>v0.3: Add smart defaults for single-job burst based on time estimates. This is where it gets riskier.</p>
<p>Learn from telemetry (with permission):</p>
<p>How often do users override? What jobs do they kill? Where are estimates wrong? The Killer Insight from Your Background You have something most tool builders don’t: economic frameworks that actually work. Your queueing theory isn’t academic - you’ve proven it with 90+ universities.</p>
<p>The tool’s intelligence isn’t “AI-powered smart scheduling.” It’s:</p>
<p>“This tool embodies 20 years of research computing economics. It knows when cloud saves you money because the math is fundamental, not heuristic.”</p>
<p>That’s defensible. That builds trust. Users might not understand queueing theory, but they understand “this guy ran UCLA’s research computing for 20 years and now works at AWS - he probably knows when cloud makes sense.”</p>
<p>Where should it make its first decision? Parallel workloads? Memory overflow? Something else?</p>
<p>I didn’t say just does it, I said just works I would recommend looking at coiled as an example for Python</p>
<p>Jan 31</p>
<p>Recalibrated understanding of “just works” versus automatic decision-making Ah, important distinction. “Just works” ≠ “decides for you.”</p>
<p>Let me look at Coiled’s model:</p>
<p>What Coiled Gets Right 1. Explicit opt-in, zero configuration</p>
<p>python # User clearly chooses cloud import coiled</p>
<p>cluster = coiled.Cluster(n_workers=50) client = Client(cluster)</p>
</div>
<div class="section level1">
<h1 id="now-normal-dask-code-just-works-on-cloud">Now normal Dask code just works on cloud<a class="anchor" aria-label="anchor" href="#now-normal-dask-code-just-works-on-cloud"></a></h1>
<p>result = df.groupby(‘key’).mean().compute() 2. Environment sync is automatic</p>
<p>Detects local packages Builds matching environment on cloud workers User doesn’t think about Docker, AMIs, etc. 3. Transparent resource management</p>
<p>User specifies workers/memory Coiled handles instance types, networking, security groups Bills by second, auto-shutdown 4. Local development stays local</p>
<p>python # Develop/test locally with small data client = Client() # local cluster</p>
</div>
<div class="section level1">
<h1 id="scale-to-cloud-when-ready---same-code">Scale to cloud when ready - same code<a class="anchor" aria-label="anchor" href="#scale-to-cloud-when-ready---same-code"></a></h1>
<p>client = Client(coiled.Cluster(n_workers=100)) R Equivalent Pattern r library(awsburst)</p>
</div>
<div class="section level1">
<h1 id="explicit-user-chooses-cloud">Explicit: user chooses cloud<a class="anchor" aria-label="anchor" href="#explicit-user-chooses-cloud"></a></h1>
<p>cluster &lt;- aws_cluster(workers = 50)</p>
</div>
<div class="section level1">
<h1 id="now-normal-futureparallel-code-just-works">Now normal future/parallel code just works<a class="anchor" aria-label="anchor" href="#now-normal-futureparallel-code-just-works"></a></h1>
<p>plan(cluster)</p>
<p>results &lt;- future_map(samples, expensive_analysis) Behind the scenes (invisible to user):</p>
<p>Spins up Fargate tasks or EC2 instances Syncs R environment (renv snapshot) Handles S3 data movement Manages authentication Tears down when done User thinks about: “I want 50 workers” User doesn’t think about: Instance types, VPCs, Docker, IAM roles, package installation</p>
<p>The “Just Works” Requirements Environment reproducibility:</p>
<p>r # First time setup - once ever aws_burst_setup()<br>
# Captures AWS credentials, creates S3 bucket, ECR repo # Maybe 2 minutes of one-time config</p>
</div>
<div class="section level1">
<h1 id="then-forever-after">Then forever after<a class="anchor" aria-label="anchor" href="#then-forever-after"></a></h1>
<p>cluster &lt;- aws_cluster(workers = 10) # Tool automatically: # - Snapshots local renv # - Builds/caches container image # - Only rebuilds if packages changed Smart defaults:</p>
<p>r # Minimal specification cluster &lt;- aws_cluster(workers = 20) # Tool picks: r5.xlarge instances (or Fargate equivalent)</p>
</div>
<div class="section level1">
<h1 id="more-control-when-needed">More control when needed<a class="anchor" aria-label="anchor" href="#more-control-when-needed"></a></h1>
<p>cluster &lt;- aws_cluster( workers = 20, memory = “16GB”, # tool picks instance family instance = “c5.4xlarge” # or explicit ) Cost visibility:</p>
<p>r cluster &lt;- aws_cluster(workers = 50) # &gt; Starting cluster: ~$3.50/hour, bursting from S3 bucket: my-burst-data # &gt; Cluster ready (52 seconds)</p>
</div>
<div class="section level1">
<h1 id="when-done">When done<a class="anchor" aria-label="anchor" href="#when-done"></a></h1>
<p>cluster$shutdown() # &gt; Cluster runtime: 23 minutes # &gt; Total cost: $1.34 What Makes It “Just Work” for R 1. Package environment sync - This is the hard part</p>
<p>Coiled has it easier (conda/pip are hermetic) R package installation is messier Solution: renv + Docker layers + caching 2. Data movement - Your expertise</p>
<p>CargoShip-powered S3 uploads Intelligent serialization (qs vs arrow vs fst) Automatic chunking for parallel tasks 3. Authentication - One-time setup</p>
<p>r aws_burst_setup() # Walks through: AWS credentials, region, S3 bucket # Stores config, never ask again 4. No babysitting</p>
<p>Auto-shutdown after idle Cost alerts if running too long Cleanup failed jobs The Tension Resolution You’re right - there’s tension between “just works” and “simple.” Coiled resolves it by:</p>
<p>Making the complex parts invisible (infrastructure) Keeping the important parts explicit (resource allocation, cost)</p>
<p>For your tool:</p>
<p>User explicitly chooses to use AWS (no surprise bills) Tool handles all AWS complexity (no learning curve) Transparent about cost and time (build trust) Does that align better with your vision? The user says “I want cloud compute” and the tool makes that trivially easy?</p>
<p>It does and I am going to call this “staRburst”</p>
<p>Tell me more about future, what it does, how it works, and why that is the integration point</p>
<p>Jan 31</p>
<p>Architected explanation of future’s role as staRburst’s integration foundation future is the backbone of modern parallel computing in R. Creating a staRburst backend for it means instant compatibility with a massive ecosystem.</p>
<p>What future Does Core concept: Separate what to compute from where/how to compute it.</p>
<p>r # Sequential (normal R) result &lt;- expensive_function(data)</p>
</div>
<div class="section level1">
<h1 id="future---same-semantics-different-execution">Future - same semantics, different execution<a class="anchor" aria-label="anchor" href="#future---same-semantics-different-execution"></a></h1>
<p>f &lt;- future({ expensive_function(data) }) result &lt;- value(f) # blocks until ready The magic: changing execution model requires zero code changes, just change the “plan”:</p>
<p>r # Run locally in background plan(multisession) f &lt;- future({ slow_work() }) # … do other stuff … result &lt;- value(f)</p>
</div>
<div class="section level1">
<h1 id="run-on-cluster">Run on cluster<a class="anchor" aria-label="anchor" href="#run-on-cluster"></a></h1>
<p>plan(cluster, workers = c(“node1”, “node2”, “node3”)) f &lt;- future({ slow_work() }) # same code! result &lt;- value(f) How It Works 1. Future objects are promises</p>
<p>r f &lt;- future({ x &lt;- expensive_computation() x * 2 }) # Returns immediately, computation happens elsewhere 2. Backends (plans) handle execution</p>
<p>sequential: Run in current R session (default, no parallelism) multisession: Spawn R workers on local machine multicore: Fork R process (Unix only) cluster: Run on remote R sessions via SSH future.batchtools: Submit to HPC schedulers (Slurm, SGE, etc.) 3. Resolving futures</p>
<p>r result &lt;- value(f) # blocks until ready 4. Automatic dependency tracking</p>
<p>r x &lt;- 10 f &lt;- future({ x + 5 }) # <code>x</code> is captured and sent to worker Why This Is Your Integration Point 1. Ecosystem compatibility</p>
<p>Tons of packages build on future:</p>
<p>furrr: future + purrr → parallel map operations r library(furrr) plan(future_starburst) # your backend</p>
<p># Now this runs on AWS automatically results &lt;- future_map(samples, expensive_analysis) future.apply: Parallel lapply, sapply, etc. r plan(future_starburst) results &lt;- future_lapply(1:1000, function(i) simulate(i)) targets: Make-like workflow tool promises: Async programming for Shiny One integration → works with entire ecosystem</p>
<ol start="2" style="list-style-type: decimal"><li>Users already know the API</li>
</ol><p>R users doing parallel work likely already use furrr or future.apply. staRburst becomes:</p>
<p>r # Change one line plan(multisession) # local plan(future_starburst, workers = 50) # AWS # All their existing code just works 3. Clean separation of concerns</p>
<p>r # Business logic (user code) analyze &lt;- function(sample) { preprocess(sample) %&gt;% fit_model() %&gt;% extract_results() }</p>
</div>
<div class="section level1">
<h1 id="execution-strategy-starburst-handles">Execution strategy (staRburst handles)<a class="anchor" aria-label="anchor" href="#execution-strategy-starburst-handles"></a></h1>
<p>plan(future_starburst) results &lt;- future_map(samples, analyze) User thinks about what to compute. staRburst handles where.</p>
<p>How future Backends Work Interface to implement:</p>
<p>r # Simplified - actual implementation more complex plan.starburst &lt;- function(workers = 10, …) { # Setup: create cluster, sync environment cluster &lt;- aws_cluster_create(workers)</p>
<p># Return plan object structure( list(cluster = cluster, workers = workers), class = c(“starburst”, “cluster”, “future”) ) }</p>
</div>
<div class="section level1">
<h1 id="send-work-to-cluster">Send work to cluster<a class="anchor" aria-label="anchor" href="#send-work-to-cluster"></a></h1>
<p>future_starburst &lt;- function(expr, envir, …) { # 1. Capture expression and dependencies # 2. Serialize to S3 # 3. Submit to Fargate/EC2 worker # 4. Return future object with result location }</p>
</div>
<div class="section level1">
<h1 id="retrieve-result">Retrieve result<a class="anchor" aria-label="anchor" href="#retrieve-result"></a></h1>
<p>value.starburst &lt;- function(future, …) { # 1. Check if result ready (poll S3 or use notifications) # 2. Download result # 3. Deserialize and return } The heavy lifting:</p>
<p>Environment capture (what objects does expr need?) Serialization (get data to workers) Worker management (spin up, monitor, shut down) Result collection (get data back) staRburst Architecture r library(starburst)</p>
</div>
<div class="section level1">
<h1 id="one-time-setup">One-time setup<a class="anchor" aria-label="anchor" href="#one-time-setup"></a></h1>
<p>starburst_setup() # AWS creds, S3 bucket, etc.</p>
</div>
<div class="section level1">
<h1 id="create-plan">Create plan<a class="anchor" aria-label="anchor" href="#create-plan"></a></h1>
<p>plan(future_starburst, workers = 20, memory = “8GB”, cpu = 4, region = “us-east-1” )</p>
</div>
<div class="section level1">
<h1 id="now-any-future-based-code-uses-aws">Now any future-based code uses AWS<a class="anchor" aria-label="anchor" href="#now-any-future-based-code-uses-aws"></a></h1>
<p>library(furrr) results &lt;- future_map(big_list, expensive_function) Behind the scenes:</p>
<p>Environment sync: Snapshot renv.lock, build Docker image, cache in ECR Cluster creation: Spin up Fargate tasks or EC2 instances Work distribution: Each future() call → task to worker Data movement: Serialize deps to S3, workers pull, execute, push results Result collection: Download results as they complete Cleanup: Auto-shutdown idle workers Why This Is Better Than Building From Scratch If you built a custom API:</p>
<p>r # Your custom thing starburst_map(data, function(x) …) Users need to:</p>
<p>Learn your API Rewrite existing code Limited to your functions With future backend:</p>
<p>r plan(future_starburst) # Now ALL future-based code works: # - furrr::future_map # - future.apply::future_lapply # - targets pipelines # - custom future code Users get:</p>
<p>Familiar API Works with existing code Entire ecosystem compatibility The Technical Challenges 1. Environment reproducibility</p>
<p>Capture local package versions (renv) Build matching Docker image Cache for reuse (most runs don’t change packages) 2. Data serialization</p>
<p>Large objects: use qs (10x faster than base R) Data frames: Arrow/Parquet for efficiency Special objects: Handle connections, pointers gracefully 3. Error handling</p>
<p>Worker crashes → retry or fail gracefully Partial results → save what completed Cost runaway → kill cluster if budget exceeded 4. Performance</p>
<p>Cold start: 30-60s for Fargate Data transfer: Use your CargoShip optimizations Task overhead: Batch small tasks to amortize startup Example Integration r library(starburst) library(furrr)</p>
</div>
<div class="section level1">
<h1 id="setup-cluster">Setup cluster<a class="anchor" aria-label="anchor" href="#setup-cluster"></a></h1>
<p>plan(future_starburst, workers = 100)</p>
</div>
<div class="section level1">
<h1 id="genomics-workflow---10000-samples">Genomics workflow - 10,000 samples<a class="anchor" aria-label="anchor" href="#genomics-workflow---10000-samples"></a></h1>
<p>samples &lt;- list.files(“s3://my-data/samples/”)</p>
<p>results &lt;- future_map(samples, function(sample_id) { # Each runs on separate Fargate task data &lt;- read_fastq(paste0(“s3://my-data/samples/”, sample_id)) aligned &lt;- align_reads(data) variants &lt;- call_variants(aligned) return(variants) }, .options = furrr_options(seed = TRUE))</p>
</div>
<div class="section level1">
<h1 id="results-collected-automatically">Results collected automatically<a class="anchor" aria-label="anchor" href="#results-collected-automatically"></a></h1>
</div>
<div class="section level1">
<h1 id="cluster-shuts-down-when-done">Cluster shuts down when done<a class="anchor" aria-label="anchor" href="#cluster-shuts-down-when-done"></a></h1>
<p>User experience:</p>
<p>Wrote normal furrr code Added plan(future_starburst) 10,000 samples processed in ~time of 1 sample Cost: $47, time: 18 minutes vs 208 hours locally Does this clarify why future is the right foundation? You’re not building a parallel computing framework - you’re building an AWS backend for the existing R parallel computing standard.</p>
<p>Completely, no question Does fargate seem like the best starting point? Seems coiled keeps it simple as well. Could get more clever with GPU, EMR etc or remote data or db</p>
<p>Jan 31</p>
<p>Validated start-simple strategy, endorsed Fargate expansion path Fargate is absolutely the right starting point. Here’s why:</p>
<p>Why Fargate for v1 1. Zero infrastructure management</p>
<p>No AMIs to build/maintain No instance lifecycle (launching, monitoring, terminating) No capacity planning Just: “run this container with this much CPU/RAM” 2. Perfect cost model for bursting</p>
<p>Pay per-second from start to stop No warm pools to maintain No minimum commitments Fits the “spin up, compute, tear down” pattern perfectly 3. Fast enough</p>
<p>30-60s cold start is acceptable for parallel workloads If you’re running 100 tasks for 20 minutes each, 45s startup is 4% overhead For the genomics example: 10,000 samples × 18 min each = startup is noise 4. Simple mental model</p>
<p>r plan(future_starburst, workers = 50, cpu = 4, memory = “8GB” ) That’s it. User doesn’t think about instance types, spot vs on-demand, availability zones, etc.</p>
<ol start="5" style="list-style-type: decimal"><li>Covers 80% of use cases</li>
</ol><p>CPU-bound parallel work: ✓ Memory up to 120GB: ✓ (covers most R workloads) No GPU: fine, most R work is CPU Runtime &lt;hours: ✓ (typical batch jobs) The Coiled Parallel Coiled started simple too:</p>
<p>Dask workers on managed containers Simple scaling knobs (workers, memory, CPU) Let users prove value before adding complexity They added sophisticated features later:</p>
<p>Software environments GPU support Adaptive scaling Cost optimization But v1 was: “Click button, get Dask cluster.”</p>
<p>staRburst v1 Scope IN scope:</p>
<p>Fargate-based workers (4-16 vCPU, 8-120GB RAM) CPU-bound parallel workloads future backend integration Automatic environment sync (renv) S3 data movement Cost tracking and auto-shutdown OUT of scope (for now):</p>
<p>GPU instances 120GB memory jobs</p>
<p>EMR integration Spark clusters Custom VPC/networking Natural Expansion Path The future backend architecture makes expansion clean:</p>
<p>r # v1: Fargate only plan(future_starburst, workers = 50)</p>
</div>
<div class="section level1">
<h1 id="v2-add-backend-selection">v2: Add backend selection<a class="anchor" aria-label="anchor" href="#v2-add-backend-selection"></a></h1>
<p>plan(future_starburst, workers = 50, backend = “fargate”) # default plan(future_starburst, workers = 10, backend = “ec2-gpu”, instance = “g4dn.xlarge”)</p>
</div>
<div class="section level1">
<h1 id="v3-smart-backend-selection">v3: Smart backend selection<a class="anchor" aria-label="anchor" href="#v3-smart-backend-selection"></a></h1>
<p>plan(future_starburst, workers = 50, gpu = TRUE) # auto-picks EC2 GPU instances plan(future_starburst, workers = 50, memory = “256GB”) # auto-picks EC2 high-mem Under the hood, same architecture:</p>
<p>Container image (from renv) Task submission S3 data movement Result collection Just swap Fargate ECS tasks for EC2 instances when needed.</p>
<p>When You’d Add EC2 GPU workloads:</p>
<p>r # User has torch/keras code plan(future_starburst, workers = 4, gpu = TRUE) # Backend: Launches g4dn.xlarge instances Memory monsters (&gt;120GB):</p>
<p>r # Genomics with 500GB reference genome plan(future_starburst, workers = 1, memory = “500GB”) # Backend: Launches r5.16xlarge Long-running (&gt;4 hours) where Spot savings matter:</p>
<p>r # Overnight simulation plan(future_starburst, workers = 100, spot = TRUE) # Backend: Spot instances with interruption handling When You’d Add EMR/Spark When users need distributed data processing, not just parallel R:</p>
<p>r # Billions of rows, distributed joins plan(future_starburst_spark, workers = 20) # Behind scenes: EMR cluster, sparklyr integration But this is different architecture - Spark != R parallelism. Probably a separate package (starburst.spark?).</p>
<p>MVP Success Criteria v1 is successful if:</p>
<p>Researcher can go from plan(multisession) to plan(future_starburst) in one line Environment “just works” (renv sync automatic) 100-way parallelism costs &lt;$10 and finishes in minutes vs hours They recommend it to colleagues You know you need v2 when:</p>
<p>Users ask for GPU support (torch/keras workflows) Memory limits hit (genomics, large matrix ops) Cost optimization requests (spot instances) Implementation Priority Week 1-2: Core infrastructure</p>
<p>AWS authentication setup S3 bucket creation ECR repository for Docker images Basic Fargate task definition Week 3-4: future backend</p>
<p>Implement plan.starburst Task submission to Fargate Basic serialization (start with qs) Result collection Week 5-6: Environment sync</p>
<p>renv snapshot capture Docker image building ECR caching (only rebuild when packages change) Week 7-8: Polish</p>
<p>Cost tracking Auto-shutdown Error handling Documentation Week 9-10: Testing with real workloads</p>
<p>Genomics pipelines Monte Carlo simulations Bootstrap resampling The Pitch staRburst v1: “Change one line of code, run your parallel R workload on 100 AWS workers. No Docker, no infrastructure, no hassle. Works with furrr, future.apply, and the entire future ecosystem.”</p>
<p>Simple. Valuable. Extensible.</p>
<p>Start with Fargate. Prove the model. GPU and fancy stuff comes later when users demand it.</p>
<p>Sound like the right plan?</p>
<p>Yes, one thing super important, quotas</p>
<p>Jan 31</p>
<p>Examined AWS quota constraints and production-readiness strategies Absolutely critical. This is where “just works” crashes into AWS reality.</p>
<p>The Quota Problem Default Fargate vCPU limits (varies by region):</p>
<p>New accounts: Often 40-100 vCPUs total Established accounts: Maybe 1000 vCPUs User tries:</p>
<p>r plan(future_starburst, workers = 100, cpu = 4) # Needs 400 vCPUs # Quota: 100 vCPUs # Result: 25 tasks launch, 75 fail with cryptic AWS error User experience: Broken. Confusing. Not “just works.”</p>
<p>staRburst Must Handle This 1. Check quotas before launching</p>
<p>r plan(future_starburst, workers = 100, cpu = 4)</p>
</div>
<div class="section level1">
<h1 id="behind-the-scenes">Behind the scenes:<a class="anchor" aria-label="anchor" href="#behind-the-scenes"></a></h1>
</div>
<div class="section level1">
<h1 id="id_--query-service-quotas-api">- Query Service Quotas API<a class="anchor" aria-label="anchor" href="#id_--query-service-quotas-api"></a></h1>
</div>
<div class="section level1">
<h1 id="id_--fargate-vcpu-quota-100">- Fargate vCPU quota: 100<a class="anchor" aria-label="anchor" href="#id_--fargate-vcpu-quota-100"></a></h1>
</div>
<div class="section level1">
<h1 id="id_--requested-400">- Requested: 400<a class="anchor" aria-label="anchor" href="#id_--requested-400"></a></h1>
</div>
<div class="section level1">
<h1 id="id_--action-">- Action: ???<a class="anchor" aria-label="anchor" href="#id_--action-"></a></h1>
<pre><code>
**2. Smart responses**

**Option A: Graceful degradation**</code></pre>
<p>⚠ Requested 100 workers (400 vCPUs) ⚠ Your Fargate quota: 100 vCPUs ⚠ Launching 25 workers instead ⚠ Increase quota: <a href="https://console.aws.amazon.com/servicequotas/" class="external-link uri">https://console.aws.amazon.com/servicequotas/</a>… ⚠ Continue? [y/n]</p>
<pre><code>
**Option B: Incremental scaling**</code></pre>
<p>⚠ Requested 100 workers (400 vCPUs) ⚠ Your Fargate quota: 100 vCPUs<br>
⚠ Launching in waves: 25 workers at a time ⚠ Tasks will queue and run as workers complete ✓ Proceeding with wave-based execution</p>
<pre><code>
**Option C: Fail fast with guidance**</code></pre>
<p>✗ Cannot launch 100 workers (400 vCPUs needed) ✗ Current Fargate vCPU quota: 100 ✗ Options: 1. Request quota increase (usually approved in 24hrs) aws service-quotas request-service-quota-increase<br>
–service-code fargate –quota-code L-3032A538<br>
–desired-value 500 2. Reduce workers: plan(future_starburst, workers = 25) 3. Switch to smaller tasks: cpu = 2 (allows 50 workers) Implementation Strategy Check quotas during setup:</p>
<p>r starburst_setup()</p>
</div>
<div class="section level1">
<h1 id="checks-and-reports">Checks and reports:<a class="anchor" aria-label="anchor" href="#checks-and-reports"></a></h1>
<p>✓ AWS credentials: valid ✓ S3 bucket: created (my-starburst-data) ✓ ECR repository: created ⚠ Fargate vCPU quota: 100 (you may want to increase this) Current quota allows ~25 workers with 4 vCPUs each Request increase: starburst_request_quota_increase() Check quotas before launch:</p>
<p>r plan(future_starburst, workers = 100, cpu = 4)</p>
</div>
<div class="section level1">
<h1 id="internally">Internally:<a class="anchor" aria-label="anchor" href="#internally"></a></h1>
<p>quota_check &lt;- get_fargate_quota() needed &lt;- workers * cpu if (needed &gt; quota_check$available) { # Handle gracefully } Helper function:</p>
<p>r starburst_quota_status() # &gt; Fargate vCPU quota: 100 / 100 used # &gt; Recommended: Request increase to 500 # &gt; # &gt; starburst_request_quota_increase(vcpus = 500) Key Quotas to Track Fargate:</p>
<p>vCPU limit (per region) Tasks per cluster Usually the bottleneck EC2 (for future GPU/spot):</p>
<p>Instance type limits (especially GPU) Spot instance limits On-demand vCPU limits S3:</p>
<p>Request rate (5500 PUT/s per prefix) Usually not a problem but good to monitor ECR:</p>
<p>Image storage (10 TB default) Pull rate limits Wave-Based Execution (Best UX) r plan(future_starburst, workers = 100, cpu = 4)</p>
</div>
<div class="section level1">
<h1 id="if-quota-only-allows-25-at-a-time">If quota only allows 25 at a time:<a class="anchor" aria-label="anchor" href="#if-quota-only-allows-25-at-a-time"></a></h1>
<p>✓ Starting cluster: 25 workers (wave 1 of 4) ⚡ [Wave 1: ████████████] 100% complete (127 tasks done) ⚡ [Wave 2: ████████░░░░] 67% complete (85 tasks done) … This way:</p>
<p>User gets all 100 workers worth of work done Just takes longer (sequential waves vs parallel) No manual intervention needed Transparent about what’s happening Quota Increase Automation r starburst_request_quota_increase( vcpus = 500, justification = “Running parallel genomics analysis” )</p>
</div>
<div class="section level1">
<h1 id="behind-scenes">Behind scenes:<a class="anchor" aria-label="anchor" href="#behind-scenes"></a></h1>
</div>
<div class="section level1">
<h1 id="id_--calls-service-quotas-api">- Calls Service Quotas API<a class="anchor" aria-label="anchor" href="#id_--calls-service-quotas-api"></a></h1>
</div>
<div class="section level1">
<h1 id="id_--creates-increase-request">- Creates increase request<a class="anchor" aria-label="anchor" href="#id_--creates-increase-request"></a></h1>
</div>
<div class="section level1">
<h1 id="id_--aws-usually-approves-in-1-24-hours-for-reasonable-requests">- AWS usually approves in 1-24 hours for reasonable requests<a class="anchor" aria-label="anchor" href="#id_--aws-usually-approves-in-1-24-hours-for-reasonable-requests"></a></h1>
</div>
<div class="section level1">
<h1 id="id_--sends-email-to-user-when-approved">- Sends email to user when approved<a class="anchor" aria-label="anchor" href="#id_--sends-email-to-user-when-approved"></a></h1>
<p>Pre-emptive guidance:</p>
<p>r starburst_setup()</p>
</div>
<div class="section level1">
<h1 id="during-setup-if-quota-is-low">During setup, if quota is low:<a class="anchor" aria-label="anchor" href="#during-setup-if-quota-is-low"></a></h1>
<p>⚠ Your Fargate quota (100 vCPUs) is low for parallel workloads ⚠ We recommend requesting an increase to 500 vCPUs ⚠ This usually takes 24 hours to approve ⚠ Request now? [y/n] Why This Matters Without quota handling:</p>
<p>User: “This doesn’t work, got weird errors” Reality: Hit quota, partial failure, confusing Outcome: Abandoned tool, bad review With quota handling:</p>
<p>User: “Asked for 100 workers, quota only allowed 25, tool told me how to fix it” Reality: Transparent, helpful, actionable Outcome: User requests quota increase, comes back, succeeds Check During CI/Testing r # In test suite test_that(“quota checks work”, { # Mock quota API quota &lt;- 100 plan &lt;- plan_starburst(workers = 50, cpu = 4) # needs 200</p>
<p>expect_warning(“quota”) expect_equal(plan$actual_workers, 25) # graceful degradation }) Documentation Critical Section README.md:</p>
<p>markdown ### AWS Quotas</p>
<p>staRburst requires sufficient AWS Fargate vCPU quota.</p>
<p><strong>Check your quota:</strong> - Visit AWS Console &gt; Service Quotas &gt; Fargate - Or run: <code><a href="reference/starburst_quota_status.html">starburst_quota_status()</a></code></p>
<p><strong>Typical quotas:</strong> - New accounts: 100 vCPUs (allows ~25 workers) - After increase: 500-1000 vCPUs (allows 100+ workers)</p>
<p><strong>Request increase:</strong> 1. Automatic: <code>starburst_request_quota_increase(vcpus = 500)</code> 2. Manual: [AWS Console instructions]</p>
<p>Approval usually takes 1-24 hours. You know this pain intimately from AWS work - users hitting quotas is the #1 support issue for any AWS tool. Handle it proactively and staRburst is 10x better than alternatives.</p>
<p>What’s your experience with quota issues at AWS? Any patterns you’ve seen for handling this gracefully?</p>
<p>Wave based for sure plus ask to make quota increase request for user</p>
<p>Jan 31</p>
<p>Validated wave-based execution strategy and proactive quota assistance approach Perfect. That’s the right UX. Let me sketch the flow:</p>
<p>Ideal User Experience First-time user, low quota:</p>
<p>r library(starburst)</p>
<p>plan(future_starburst, workers = 100, cpu = 4)</p>
<pre><code></code></pre>
<p>⚡ Checking Fargate capacity… ⚠ Requested: 100 workers (400 vCPUs) ⚠ Current quota: 100 vCPUs (allows 25 workers max)</p>
<p>📋 Execution plan: • Running in 4 waves of 25 workers each • Wave 1: starts immediately • Waves 2-4: start as workers complete • Estimated total time: ~same as 100 parallel + 3x startup overhead</p>
<p>💡 Quota increase recommended: Request 500 vCPU quota? (usually approved in 1-24 hours) [y/n]: y</p>
<p>✓ Quota increase requested (Case ID: 12345678) ✓ You’ll receive email when approved ✓ Future runs will use full parallelism</p>
<p>⚡ Starting wave 1 (25 workers)… User just says yes once, problem solved forever.</p>
<p>Implementation Quota check with automatic fallback:</p>
<p>r plan.starburst &lt;- function(workers = 10, cpu = 4, memory = “8GB”, …) {</p>
<p># Check quota quota_info &lt;- check_fargate_quota() vcpus_needed &lt;- workers * cpu vcpus_available &lt;- quota_info$limit</p>
<p>if (vcpus_needed &gt; vcpus_available) { # Calculate wave-based execution workers_per_wave &lt;- floor(vcpus_available / cpu) num_waves &lt;- ceiling(workers / workers_per_wave)</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode R"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="co"># Inform user</span></span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="fu">cat_info</span>(<span class="fu">sprintf</span>(</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>  <span class="st">"⚠ Requested: %d workers (%d vCPUs)</span><span class="sc">\n</span><span class="st">⚠ Current quota: %d vCPUs (allows %d workers max)</span><span class="sc">\n</span><span class="st">"</span>,</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>  workers, vcpus_needed, vcpus_available, workers_per_wave</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>))</span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a><span class="fu">cat_info</span>(<span class="fu">sprintf</span>(</span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a>  <span class="st">"</span><span class="sc">\n</span><span class="st">📋 Execution plan:</span><span class="sc">\n</span><span class="st">   • Running in %d waves of %d workers each</span><span class="sc">\n</span><span class="st">"</span>,</span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a>  num_waves, workers_per_wave</span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a>))</span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a><span class="co"># Offer quota increase</span></span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span>quota_info<span class="sc">$</span>increase_pending <span class="sc">&amp;&amp;</span> <span class="fu">interactive</span>()) {</span>
<span id="cb13-14"><a href="#cb13-14" tabindex="-1"></a>  recommended_quota <span class="ot">&lt;-</span> <span class="fu">suggest_quota</span>(vcpus_needed)</span>
<span id="cb13-15"><a href="#cb13-15" tabindex="-1"></a>  </span>
<span id="cb13-16"><a href="#cb13-16" tabindex="-1"></a>  <span class="fu">cat_info</span>(<span class="fu">sprintf</span>(</span>
<span id="cb13-17"><a href="#cb13-17" tabindex="-1"></a>    <span class="st">"</span><span class="sc">\n</span><span class="st">💡 Quota increase recommended:</span><span class="sc">\n</span><span class="st">   Request %d vCPU quota? (usually approved in 1-24 hours)</span><span class="sc">\n</span><span class="st">"</span>,</span>
<span id="cb13-18"><a href="#cb13-18" tabindex="-1"></a>    recommended_quota</span>
<span id="cb13-19"><a href="#cb13-19" tabindex="-1"></a>  ))</span>
<span id="cb13-20"><a href="#cb13-20" tabindex="-1"></a>  </span>
<span id="cb13-21"><a href="#cb13-21" tabindex="-1"></a>  response <span class="ot">&lt;-</span> <span class="fu">readline</span>(<span class="st">"   [y/n]: "</span>)</span>
<span id="cb13-22"><a href="#cb13-22" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">tolower</span>(response) <span class="sc">==</span> <span class="st">"y"</span>) {</span>
<span id="cb13-23"><a href="#cb13-23" tabindex="-1"></a>    case_id <span class="ot">&lt;-</span> <span class="fu">request_quota_increase</span>(</span>
<span id="cb13-24"><a href="#cb13-24" tabindex="-1"></a>      <span class="at">quota_code =</span> <span class="st">"L-3032A538"</span>,  <span class="co"># Fargate vCPU</span></span>
<span id="cb13-25"><a href="#cb13-25" tabindex="-1"></a>      <span class="at">desired_value =</span> recommended_quota,</span>
<span id="cb13-26"><a href="#cb13-26" tabindex="-1"></a>      <span class="at">reason =</span> <span class="fu">sprintf</span>(<span class="st">"Parallel R computing with %d workers"</span>, workers)</span>
<span id="cb13-27"><a href="#cb13-27" tabindex="-1"></a>    )</span>
<span id="cb13-28"><a href="#cb13-28" tabindex="-1"></a>    <span class="fu">cat_success</span>(<span class="fu">sprintf</span>(<span class="st">"✓ Quota increase requested (Case ID: %s)</span><span class="sc">\n</span><span class="st">"</span>, case_id))</span>
<span id="cb13-29"><a href="#cb13-29" tabindex="-1"></a>    <span class="fu">cat_success</span>(<span class="st">"✓ You'll receive email when approved</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb13-30"><a href="#cb13-30" tabindex="-1"></a>  }</span>
<span id="cb13-31"><a href="#cb13-31" tabindex="-1"></a>}</span>
<span id="cb13-32"><a href="#cb13-32" tabindex="-1"></a></span>
<span id="cb13-33"><a href="#cb13-33" tabindex="-1"></a><span class="co"># Create wave-based plan</span></span>
<span id="cb13-34"><a href="#cb13-34" tabindex="-1"></a>plan <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb13-35"><a href="#cb13-35" tabindex="-1"></a>  <span class="at">workers =</span> workers,</span>
<span id="cb13-36"><a href="#cb13-36" tabindex="-1"></a>  <span class="at">workers_per_wave =</span> workers_per_wave,</span>
<span id="cb13-37"><a href="#cb13-37" tabindex="-1"></a>  <span class="at">num_waves =</span> num_waves,</span>
<span id="cb13-38"><a href="#cb13-38" tabindex="-1"></a>  <span class="at">cpu =</span> cpu,</span>
<span id="cb13-39"><a href="#cb13-39" tabindex="-1"></a>  <span class="at">memory =</span> memory,</span>
<span id="cb13-40"><a href="#cb13-40" tabindex="-1"></a>  <span class="at">quota_limited =</span> <span class="cn">TRUE</span></span>
<span id="cb13-41"><a href="#cb13-41" tabindex="-1"></a>)</span></code></pre></div>
<p>} else { # Sufficient quota plan &lt;- list( workers = workers, workers_per_wave = workers, num_waves = 1, cpu = cpu, memory = memory, quota_limited = FALSE ) }</p>
<p>structure(plan, class = c(“starburst”, “cluster”, “future”)) } Wave Execution When tasks are submitted:</p>
<p>r # User code results &lt;- future_map(samples, expensive_analysis)</p>
</div>
<div class="section level1">
<h1 id="behind-the-scenes-1">Behind the scenes<a class="anchor" aria-label="anchor" href="#behind-the-scenes-1"></a></h1>
<p>submit_tasks_with_waves &lt;- function(tasks, plan) { if (plan$quota_limited) { # Queue-based execution task_queue &lt;- tasks results &lt;- list()</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode R"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="cf">for</span> (wave <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>plan<span class="sc">$</span>num_waves) {</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>  <span class="fu">cat_info</span>(<span class="fu">sprintf</span>(<span class="st">"⚡ Starting wave %d of %d (%d workers)...</span><span class="sc">\n</span><span class="st">"</span>, </span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>                  wave, plan<span class="sc">$</span>num_waves, plan<span class="sc">$</span>workers_per_wave))</span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a>  </span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a>  <span class="co"># Launch workers for this wave</span></span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a>  wave_tasks <span class="ot">&lt;-</span> <span class="fu">head</span>(task_queue, plan<span class="sc">$</span>workers_per_wave)</span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a>  task_queue <span class="ot">&lt;-</span> <span class="fu">tail</span>(task_queue, <span class="sc">-</span>plan<span class="sc">$</span>workers_per_wave)</span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a>  </span>
<span id="cb14-9"><a href="#cb14-9" tabindex="-1"></a>  <span class="co"># Submit to Fargate</span></span>
<span id="cb14-10"><a href="#cb14-10" tabindex="-1"></a>  wave_futures <span class="ot">&lt;-</span> <span class="fu">lapply</span>(wave_tasks, submit_to_fargate)</span>
<span id="cb14-11"><a href="#cb14-11" tabindex="-1"></a>  </span>
<span id="cb14-12"><a href="#cb14-12" tabindex="-1"></a>  <span class="co"># Wait for wave to complete</span></span>
<span id="cb14-13"><a href="#cb14-13" tabindex="-1"></a>  wave_results <span class="ot">&lt;-</span> <span class="fu">lapply</span>(wave_futures, value)</span>
<span id="cb14-14"><a href="#cb14-14" tabindex="-1"></a>  results <span class="ot">&lt;-</span> <span class="fu">c</span>(results, wave_results)</span>
<span id="cb14-15"><a href="#cb14-15" tabindex="-1"></a>  </span>
<span id="cb14-16"><a href="#cb14-16" tabindex="-1"></a>  <span class="co"># Progress</span></span>
<span id="cb14-17"><a href="#cb14-17" tabindex="-1"></a>  <span class="fu">cat_success</span>(<span class="fu">sprintf</span>(<span class="st">"✓ Wave %d complete (%d/%d tasks done)</span><span class="sc">\n</span><span class="st">"</span>,</span>
<span id="cb14-18"><a href="#cb14-18" tabindex="-1"></a>                     wave, <span class="fu">length</span>(results), <span class="fu">length</span>(tasks)))</span>
<span id="cb14-19"><a href="#cb14-19" tabindex="-1"></a>}</span></code></pre></div>
<p>} else { # Normal parallel execution # All tasks at once } } Smart Quota Suggestions r suggest_quota &lt;- function(needed_vcpus) { # Round up to reasonable increments # AWS likes: 100, 200, 500, 1000, 2000, 5000</p>
<p>if (needed_vcpus &lt;= 200) return(200) if (needed_vcpus &lt;= 500) return(500) if (needed_vcpus &lt;= 1000) return(1000) if (needed_vcpus &lt;= 2000) return(2000) return(5000) } Reasoning: Ask for more than you need now, so you don’t hit the limit again soon. If they need 400, request 500.</p>
<p>Non-Interactive Mode For scripts/automation:</p>
<p>r # In automated script plan(future_starburst, workers = 100, auto_quota_request = TRUE) # Don’t prompt, just request</p>
</div>
<div class="section level1">
<h1 id="or">Or<a class="anchor" aria-label="anchor" href="#or"></a></h1>
<p>options(starburst.auto_quota = TRUE) plan(future_starburst, workers = 100) Quota Increase Request Function r request_quota_increase &lt;- function(quota_code, desired_value, reason = ““) { tryCatch({ # AWS Service Quotas API response &lt;- service_quotas_client$request_service_quota_increase( ServiceCode =”fargate”, QuotaCode = quota_code, DesiredValue = desired_value )</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode R"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="co"># Store case ID for tracking</span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>case_id <span class="ot">&lt;-</span> response<span class="sc">$</span>RequestedQuota<span class="sc">$</span>CaseId</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a><span class="co"># Save to local config</span></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a><span class="fu">save_quota_request</span>(case_id, desired_value)</span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a><span class="fu">return</span>(case_id)</span></code></pre></div>
<p>}, error = function(e) { cat_error(“Failed to request quota increase”) cat_error(“Please request manually: <a href="https://console.aws.amazon.com/servicequotas/%5Cn" class="external-link uri">https://console.aws.amazon.com/servicequotas/\n</a>”) return(NULL) }) } Check Pending Requests r check_fargate_quota &lt;- function() { # Current quota quota_response &lt;- service_quotas_client$get_service_quota( ServiceCode = “fargate”, QuotaCode = “L-3032A538” )</p>
<p>current_limit &lt;- quota_response<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mi>u</mi><mi>o</mi><mi>t</mi><mi>a</mi></mrow><annotation encoding="application/x-tex">Quota</annotation></semantics></math>Value</p>
<p># Check for pending increases pending_response &lt;- service_quotas_client$list_requested_service_quota_change_history_by_quota( ServiceCode = “fargate”, QuotaCode = “L-3032A538”, Status = “PENDING” )</p>
<p>has_pending &lt;- length(pending_response$RequestedQuotas) &gt; 0</p>
<p>list( limit = current_limit, increase_pending = has_pending ) } Setup Wizard Enhancement r starburst_setup &lt;- function() { cat_header(“staRburst Setup”)</p>
<p># … AWS credentials, S3, ECR setup …</p>
<p># Check quota proactively quota_info &lt;- check_fargate_quota()</p>
<p>cat_info(sprintf(“📊 Current Fargate vCPU quota: %d”, quota_info$limit))</p>
<p>if (quota_info<span class="math inline">$limit &lt; 500) {
    cat_info("\n💡 For typical parallel workloads, we recommend 500+ vCPUs\n")
    cat_info(sprintf("   Your current quota (%d) allows ~%d workers with 4 vCPUs each\n",
                    quota_info$</span>limit, floor(quota_info$limit / 4)))</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode R"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span>quota_info<span class="sc">$</span>increase_pending <span class="sc">&amp;&amp;</span> <span class="fu">interactive</span>()) {</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>  response <span class="ot">&lt;-</span> <span class="fu">readline</span>(<span class="st">"</span><span class="sc">\n</span><span class="st">Request quota increase to 500 vCPUs now? [y/n]: "</span>)</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">tolower</span>(response) <span class="sc">==</span> <span class="st">"y"</span>) {</span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>    <span class="fu">request_quota_increase</span>(<span class="st">"L-3032A538"</span>, <span class="dv">500</span>, <span class="st">"staRburst parallel computing"</span>)</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>    <span class="fu">cat_success</span>(<span class="st">"✓ Quota increase requested</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a>  }</span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a>}</span></code></pre></div>
<p>} else { cat_success(sprintf(“✓ Quota is sufficient (%d vCPUs)”, quota_info$limit)) } } User-Facing Documentation README:</p>
<p>markdown ### Scaling Beyond Quota Limits</p>
<p>staRburst automatically handles AWS quota limitations:</p>
<p><strong>Wave-based execution:</strong> - If you request 100 workers but quota allows 25 - staRburst runs in 4 waves of 25 workers - All work completes, just takes slightly longer - No manual intervention needed</p>
<p><strong>Automatic quota increases:</strong> - staRburst will offer to request quota increases - Usually approved within 1-24 hours - Future runs will use full parallelism</p>
<p><strong>Manual quota check:</strong></p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="reference/starburst_quota_status.html">starburst_quota_status</a></span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p>This handles the single biggest UX failure mode gracefully and proactively. Users get their work done immediately (wave-based) AND the problem gets solved permanently (quota increase).</p>
<p>Sound good?</p>
<p>Create a set of artifacts for this project</p>
<p>Jan 31</p>
<p>Architected comprehensive R package artifacts for AWS computation bursting I’ll create a comprehensive set of artifacts for the staRburst project. Let me build out the key documentation and implementation files.</p>
<p>Assembled comprehensive artifacts for project delivery I’ve created a comprehensive set of artifacts for the staRburst project. Here’s what you have:</p>
<p>Core Documentation README.md - User-facing documentation with:</p>
<p>Quick start guide Installation instructions Usage examples with furrr, future.apply Cost management features Quota handling explanation Comparison to alternatives ARCHITECTURE.md - Technical design document covering:</p>
<p>System architecture diagrams Component deep dive (client library, environment sync, task execution) Quota management strategy Data flow optimization Security model Performance benchmarks ROADMAP.md - Implementation plan with:</p>
<p>10 development phases (MVP through production-ready) 20-week timeline to v1.0 Post-v1.0 features (GPU, Spot, EMR) Risk mitigation strategies Success metrics Package Structure DESCRIPTION - R package metadata with dependencies:</p>
<p>future, paws (AWS SDK), qs, renv, arrow Proper versioning and license info Core Implementation R/plan-starburst.R - The heart of the future backend:</p>
<p>plan.starburst() - Sets up execution plan with quota checking future_starburst() - Creates futures on AWS value.starburst_future() - Retrieves results Wave-based execution logic Cost estimation R/setup.R - One-time configuration:</p>
<p>starburst_setup() - Interactive setup wizard AWS resource creation (S3, ECR, ECS, VPC) Configuration persistence starburst_config() - User preferences R/quota.R - Quota management:</p>
<p>check_fargate_quota() - Query current limits starburst_request_quota_increase() - Automated requests starburst_quota_status() - User-friendly status Wave execution calculations User Guide vignettes/getting-started.Rmd - Complete tutorial with:</p>
<p>Real-world examples (Monte Carlo, bootstrap, genomics) Data handling patterns Cost management Troubleshooting Best practices Key Design Decisions Captured Fargate as starting point - Simplicity over EC2 complexity Wave-based quota handling - Graceful degradation, automatic quota requests future backend integration - Instant ecosystem compatibility Cost transparency - Always show estimates and actuals renv for environment sync - Reproducible, cacheable The project structure is ready for development. You can start with Phase 1 (MVP) and build iteratively. The architecture supports your vision of “seamless, brain-dead simple, just works” while handling the real-world complexity of quotas and AWS infrastructure.</p>
<p>Readme Document · MD Architecture Document · MD Roadmap Document · MD DESCRIPTION</p>
<p>Plan starburst Code · R Setup Code · R Quota Code · R Getting started RMD</p>
<p>Claude is AI and can make mistakes. Please double-check responses.</p>
</div>


  </main></div>


    <footer><div class="pkgdown-footer-left">
  <p>staRburst is developed by <a href="https://github.com/scttfrdmn" class="external-link">Scott Friedman</a>.</p><p>Contact: <a href="mailto:help@starburst.ing">help@starburst.ing</a></p>
</div>

<div class="pkgdown-footer-right">
  <p>Built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> and powered by <a href="https://paws-r.github.io/" class="external-link">paws</a>.</p>
</div>

    </footer></div>





  </body></html>

